---
title: "Day 1: Basics of Statistical Power"
subtitle: "Power Workshop with R, lme4, and simR"
author: "Power Workshop"
format:
  revealjs:
    theme: default
    slide-number: true
    chalkboard: true
    preview-links: auto
    footer: "Day 1: Basics of Statistical Power"
execute:
  echo: true
  eval: true
  warning: false
  message: false
---

# Part 1: What is Statistical Power?

## Definition

**Statistical power** is the probability that a statistical test will correctly reject a false null hypothesis.

In other words: the probability of detecting an effect *when there truly is one*.

$$\text{Power} = 1 - \beta$$

Where $\beta$ is the probability of a Type II error (false negative).

## Types of Errors

|                     | H₀ True (No Effect) | H₀ False (Effect Exists) |
|---------------------|---------------------|--------------------------|
| **Reject H₀**       | Type I Error (α)    | Correct! (Power)         |
| **Fail to Reject H₀**| Correct!           | Type II Error (β)        |


## Why is power important?

- Reviewers ask for power analyses!
- Power determines whether a study can produce meaningful results.
- Low power increases the risk of false negatives
- But it also affects the reliability of significant findings (Type M and Type S errors).
  - Type M error: Magnitude of effect size is exaggerated
  - Type S error: Sign of effect is incorrect

## Why is power important?

- This is because with low power, only the largest observed effects reach significance.

::: {.callout-important}
Consequences: In a field full of underpowered studies, null effects are uninterpretable, we will have a lot of unpublished false negative effects, published effects will be overestimated, and we occasionally get effects that make no sense (wrong sign). Sound familiar?
:::

## Consequences of not considering power

- **Underpowered studies** waste resources and may miss real effects
- **Overpowered studies** waste resources (though less problematic)
- Ethical considerations: participant time and effort
- Funding agencies increasingly require power analyses
- Helps with study planning and resource allocation


## The Four Key Components

Power analysis involves four interrelated quantities:

1. **Effect size** - How large is the effect?
2. **Sample size** (N) - How many participants/observations?
3. **Alpha level** ($\alpha$) - Significance threshold (typically 0.05)
4. **Power** ($1 - \beta$) - Probability of detecting the effect

::: {.callout-note}
Given any three of these, we can calculate the fourth.
:::

## Power calculations

- Two main approaches:
  1. **Closed-form solutions**: Solve mathematical formulas for simple designs (t-tests, ANOVA, very simple LMM designs: Westfall, 2016)
  2. **Simulation-based methods**: Generate simulated data under specified conditions, run analyses. Repeat many times and count how many times null hypothesis is rejected
  
# Part 2: Power Analysis - Closed-Form

## Effect size: Cohen's d

For comparing two means:

$$d = \frac{\bar{X}_1 - \bar{X}_2}{s_{pooled}}$$

**Cohen's conventions:**

- Small: d = 0.2

- Medium: d = 0.5

- Large: d = 0.8

## Effect Size in R

```{r}
#| label: effect-size-calc
# Calculate Cohen's d manually
group1 <- c(23, 25, 28, 24, 26)
group2 <- c(20, 22, 21, 19, 23)

mean_diff <- mean(group1) - mean(group2)
pooled_sd <- sqrt((var(group1) + var(group2)) / 2)
cohens_d <- mean_diff / pooled_sd

cat("Cohen's d =", round(cohens_d, 3))
```

## Using the effectsize Package

```{r}
#| label: effectsize-package
# install.packages("effectsize")
library(effectsize)

# Calculate Cohen's d
cohens_d(group1, group2)
```

## Calculate power for the paired t-test by hand
- Example:
  - Effect size d = 0.5
  - Alpha = 0.05
  - Sample size per group: n = 32
  
```{r}
#| label: power-calculation-manual
#| echo: false
#| results: "asis"

library(stats)
d <- 0.5
n <- 32
alpha <- 0.05
non_central_param <- d * sqrt(n)
critical_t <- qt(1 - alpha / 2, df = n - 1)
power <- 1 - pt(critical_t, df = n - 1, ncp = non_central_param) + pt(-critical_t, df = n - 1, ncp = non_central_param)
cat("Calculated Power =", round(power, 4))
```

## The pwr Package

```{r}
#| label: pwr-package
# install.packages("pwr")
library(pwr)
```

The `pwr` package provides functions for:

- t-tests: `pwr.t.test()`
- ANOVA: `pwr.anova.test()`
- Correlations: `pwr.r.test()`
- Chi-square: `pwr.chisq.test()`
- And more...

## Same example as above with pwr

- Effect size d = 0.5
- Alpha = 0.05
- Sample size per group: n = 32

```{r}
#| label: power-calculation-pwr
#| echo: false
#| results: "asis"

result <- pwr.t.test(
  n = 32,            # Sample size per group
  d = 0.5,          # Effect size
  sig.level = 0.05, # Alpha
  type = "paired"   # Paired t-test
)

result
```


## Power for Independent t-test

```{r}
#| label: power-t-test
# Calculate required sample size for d = 0.5, power = 0.80
pwr.t.test(
  d = 0.5,           # Effect size
  sig.level = 0.05,  # Alpha
  power = 0.80,      # Desired power
  type = "two.sample"
)
```

## Power Curve

```{r}
#| label: power-curve
#| fig-width: 8
#| fig-height: 5
#| output-location: slide
# Create a power curve
sample_sizes <- seq(10, 200, by = 5)
powers <- sapply(sample_sizes, function(n) {
  pwr.t.test(n = n, d = 0.5, sig.level = 0.05, type = "two.sample")$power
})


# plot the power curve
plot(sample_sizes, powers, type = "l", lwd = 2,
     xlab = "Sample Size (per group)", ylab = "Power",
     main = "Power Curve for d = 0.5")
abline(h = 0.80, lty = 2, col = "red")
abline(v = sample_sizes[which.min(abs(powers - 0.80))], lty = 2, col = "blue")
```

## Power curves for different effect sizes

```{r}
#| label: power-multiple-d
#| fig-width: 8
#| fig-height: 5
#| output-location: slide


effect_sizes <- c(0.2, 0.5, 0.8)
colors <- c("blue", "green", "red")

plot(NULL, xlim = c(10, 200), ylim = c(0, 1),
     xlab = "Sample Size (per group)", ylab = "Power")

for (i in seq_along(effect_sizes)) {
  powers <- sapply(sample_sizes, function(n) {
    pwr.t.test(n = n, d = effect_sizes[i], sig.level = 0.05, type = "two.sample")$power
  })
  lines(sample_sizes, powers, col = colors[i], lwd = 2)
}

legend("bottomright", legend = paste("d =", effect_sizes), 
       col = colors, lwd = 2)
abline(h = 0.80, lty = 2)
```


# Part 4: Simulation-Based Power Analysis

## Why Simulation?

Closed-form solutions work well for simple designs, but:

- Many real designs are complex
- Mixed-effects models don't have closed-form power solutions
- Simulations can handle any design
- Provide intuition about the testing process

## Basic Simulation Approach

1. Define the "true" population parameters
2. Generate many datasets from this population
3. Analyze each dataset with your statistical test
4. Calculate the proportion of significant results

## Simulating a t-test

```{r}
#| label: simulate-t-test
set.seed(123)

simulate_power <- function(n_per_group, effect_size, n_sims = 1000) {
  significant <- replicate(n_sims, {
    # Generate data
    group1 <- rnorm(n_per_group, mean = 0, sd = 1)
    group2 <- rnorm(n_per_group, mean = effect_size, sd = 1)
    
    # Run test
    p_value <- t.test(group1, group2)$p.value
    
    # Is it significant?
    p_value < 0.05
  })
  
  mean(significant)
}

# Example
simulate_power(n_per_group = 64, effect_size = 0.5)
```

## Comparing Simulation to Closed-Form

```{r}
#| label: compare-methods
# Closed-form
closed_form <- pwr.t.test(n = 64, d = 0.5, sig.level = 0.05)$power

# Simulation
set.seed(42)
simulated <- simulate_power(n_per_group = 64, effect_size = 0.5, n_sims = 5000)

cat("Closed-form power:", round(closed_form, 4), "\n")
cat("Simulated power:", round(simulated, 4))
```


## Power Curve via Simulation

```{r}
#| label: power-curve-simulation
#| fig-width: 8
#| fig-height: 5
#| output-location: slide
sample_sizes <- seq(20, 100, by = 10)

set.seed(123)
sim_powers <- sapply(sample_sizes, function(n) {
  simulate_power(n_per_group = n, effect_size = 0.5, n_sims = 1000)
})

plot(sample_sizes, sim_powers, type = "b", pch = 16,
     xlab = "Sample Size (per group)", ylab = "Power",
     main = "Power Curve via Simulation (d = 0.5)")
abline(h = 0.80, lty = 2, col = "red")
```

# Part 5: Practical Considerations

## Sensitivity Analysis

Don't just pick one effect size! Explore a range:

```{r}
#| label: sensitivity
effect_sizes <- seq(0.2, 0.8, by = 0.1)
n <- 50  # Fixed sample size

powers <- sapply(effect_sizes, function(d) {
  pwr.t.test(n = n, d = d, sig.level = 0.05)$power
})

data.frame(effect_size = effect_sizes, power = round(powers, 3))
```

## Choosing an Effect Size

Options for determining a reasonable effect size:

1. **Literature review** - What have previous studies found?
2. **Pilot data** - Run a small pilot study
3. **Smallest effect of interest** - What's the minimum meaningful effect?
4. **Expert judgment** - Domain expertise

::: {.callout-warning}
Avoid using the same data for both effect size estimation and hypothesis testing!
:::


# Interpreting results from NHST
## Question 1

- Let’s assume you perform 40 statistical tests with $\alpha = .05$. 
    - What is the Type I error rate for 
each individual test? 
    - Assuming that the null hypotheses of all these tests are actually true, what is the probability that at least one of the 40 tests will give you a false positive result? 

# Question 2 
- Miller and Jones, working in Massachusetts, USA, publish an experiment on a new method 
for reducing prejudice, with 30 participants in each of two groups, experimental and control. 
- They obtain a significant difference in prejudice scores between the two groups, significant 
by (two-tailed) *t*-test, *p* = 0.04. You and decide to follow up their 
work.
- Before adding modifications to their procedure, you initially attempt as exact a 
replication as you can, but you want to have the best possible chance to find the effect. How many participants should you test?
    - First, make a guess, 
then calculate it (adapted from Dienes, 2008, p. 64).  

# Question 2 (Hints)
    - Use the pwr package for this one, specifically the pwr.t.test() function. You should aim for decent power (typically power = 0.8).
    - Missing some parameters for pwr.t.test()? You can calculate the effect size (Cohen’s *d*) 
for a t-test like this: $d=\frac{2t}{\sqrt{df}}$ where *t* is the t-value and *df* is the degrees of freedom of the test.
    - Don't have the t-value? You can reverse-engineer the t-value from the reported p-value ($0.04$) using the qt() function in R.
    - Don’t forget that this is a two-tailed test! 
    - Remember, a between subjects t-test has $df=n_1 + n_2 − 2$ degrees of freedom, where $n_1$ is the number of observations in Group 1 and $n_2$ is the number of observations in Group 2



# Question 3 
- Presume that like Miller and Jones you ran 30 participants in each group. You obtain a non-significant result in the same direction, *t* = 1.24 (p = 0.22). Should you 
    - try to find an explanation for the difference between the two studies?  
    - regard the Smith and Jones result as now thrown into doubt; you should reduce 
your confidence in the effectiveness of their method for overcoming prejudice?  
    - Do another study with more participants?



# Question 4 
- You read a review of studies looking at whether meditation reduces depression. 
- One hundred studies have been run and 50 are significant in the right direction and the remainder are non-significant. 
    - What should you conclude? 
    - If the null hypothesis were true, how many studies would be significant? How many would be significant in the right direction? (adapted from Dienes, 2008, p. 66) 
 


## Summary

Today we learned:

- Power is the probability of detecting a true effect
- Four key components: effect size, N, α, and power
- Closed-form solutions (pwr package) for simple designs
- Simulation approach for flexible power analysis

## Next Steps

Tomorrow (Day 2): We'll explore:

- Mixed-effects models with lme4
- Eye-tracking data analysis
- What factors affect power in complex designs


## Answer 1

- For each individual test, the Type I error rate is the same as the $\alpha = .05$.
- For all the tests together, we can first calculate the probability that none of the tests will give you a false positive result.
- For one test: $P(\text{no false positive}) = 1-P(\text{false positive}) = 1-.05=.95$
- For two tests: $P(\text{no false positives}) = .95\cdot.95=.95^2=`r .95^2`$
- For three tests: $P(\text{no false positives}) =.95^3 = `r .95^3`$
- For 40 tests: $P(\text{no false positives}) =.95^{40} = `r .95^40`$
- For the opposite probability of having at least one false positive we just subtract this from 1: $P(\text{false positives}) = 1 - P(\text{no false positives} = 1 - `r .95^40` = `r 1-.95^40`$
    - So the probability of at least one false positive is `r round(1-.95^40,4)` or `r round(100*(1-.95^40),2)`%.


# Answer 2

- The question asks you to do a power analysis to calculate how many participants you need in order to have a power of .8 to reject the null hypothesis, assuming that the effect is of the same size that the original researchers found
- But what is the effect size? Often, researchers just report test statistics
- The hints give you a formula to convert a *t*-value into an effect size (Cohen's *d*): $d=\frac{2t}{\sqrt{df}}$
    - We can figure out the degrees of freedom (see hint): $df=n_1 + n_2 − 2 = 30 + 30 - 2 = 58$
    - But we don't have the t-value.
        - With the degrees of freedom, we can get the t-value from the p-value (.04)
        - Excel can do this: `=T.INV(.02,58)` gives us `r qt(.02,58)`
        - Why .02? Because it's a two-tailed test, so .02 corresponds to 2% on each tail for a total of 4%
        - Excel has a shortcut so you don't have to think as much: `=T.INV.2T(.04,58)` gives us `r qt(.02,58, lower.tail = FALSE)`
        - The sign (positive/negative) doesn't really matter here, we just care about the absolute size of the effect.
        
# Answer 2 (continued)

- We now have the t-value (-2.1 or 2.1, it doesn't matter)
- Now we can calculate Cohen's *d* using the formula: $d=\frac{2t}{\sqrt{df}} = \frac{2\cdot2.1}{\sqrt{58}} = \frac{4.2}{\sqrt{58}} = `r 4.2/sqrt(58)`$
- This would be classified as a "medium" effect size
- Now use the effect size in G*Power to calculate the required sample size
    - Test family: `t tests`
    - Statistical test: `Means: Difference between two independent means (two groups)`
    - Type of power analysis: `A priori: Compute required effect size - given ` $\alpha$`, power, and effect size`
    - Tail(s): `two`
    - Effect size (d): ``r round(4.2/sqrt(58),2)`` (or ``r round(-4.2/sqrt(58),2)``, the result is the same)
    - $\alpha$ error prob: `.05`
    - Power ($1-\beta$ err prob): `.8` (you can go higher, if you want, but .8 is the minimum power that should be used)
    - Allocation ratio (N2/N1): `1` (because both groups are the same size)
- For a desired power of .8, this will give you a sample size of 106 or 53 per group.

# Answer 3
- A non-significant result in the same direction as the effect you were trying to replicate doesn't tell you much
    - It is definitely not (necessarily) evidence in favour of the null hypothesis!
    - Remember, **you cannot accept the null hypothesis**.
    - So don't interpret your results as if one study had found an effect and the other didn't.
    - Determining what is evidence in favour of a null effect and what isn't is very difficult in NHST
        - If your test had high power and failed to reject the $H_0$, that might be seen as evidence in favour of the null
        - But in this scenario you only tested 60 participants when we know from the previous answer that you would need at least 106 for minimally acceptable power
    - In Bayesian Statistics, you can calculate a Bayes Factor which can help you quantify the evidence for and against the $H_0$

# Answer 4

- If you were tempted to say "can't tell" or "evidence in favour of the null", that's not correct
- All the studies show an effect in the right direction, it's just that some are not significant
- If the null hypothesis is false, but we have a power of only .5, this is exactly the kind of result we would find.
- If the null hypothesis is true, then we would expect to find (assuming there was no p-hacking or similar issues) 5 significant (false positive studies), and the effects are equally likely to be in one direction or the other


